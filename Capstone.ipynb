{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone\n",
    "## *Show Me the Monet!*\n",
    "### *Or how I learned to stop worrying, and apply neural networks to artistic recognition.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The goal of the capstone was to generate a method for classifying Monet paintings versus other paintings present. Data that was used was for this analysis is stored on an s3 bucket  \n",
    "  \n",
    "The focus of this final project was to gain an understanding of neural networks that was not covered during my time at General Assembly. This required a large amount of learning with respect to neural networks and how they are used in image classification. \n",
    "\n",
    "To begin the project needed to decide what set up to use. Settled on Keras on top of theano. Had some experience with Theano from Daniel Nouri's fantastic [facial analysis](http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/) work. Additional goals for the project were to determine what key features were identified by the model in classifying Monets versus other works. \n",
    "\n",
    "The stretch objective of this project was also to be able to use the neural network to create photos that have been manipulated to appear like works of Monet. This may require additional work following the end of the course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential        # Linear stack of 'neurons'\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from keras.layers import (                 # Flattened list of layers\n",
    "    \n",
    "                          Convolution2D,   # Filters via windows of 2d input\n",
    "                          MaxPooling2D,    # Subsets image into matrices, largest value of matrix is taken\n",
    "                          Activation,      # Filters tha activate when deisred features are identified\n",
    "    \n",
    "                          Dropout,         # Prevents overfitting by dropping based on probability 1-p\n",
    "                          Flatten,         # Layers comprising model graph\n",
    "                          Dense            # Specify input argument shape\n",
    "    \n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Below is a quick demonstration of how the image processing works. A photo that is fed through the below features will have a variety of transformations applied to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Data Cleaning and Preparation\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "\n",
    "\n",
    "d_gen = ImageDataGenerator( \n",
    "                            rotation_range=0.15,   # Random Roation within specified range\n",
    "                            width_shift_range=0.15, # Horizontal shift as a fractional of image width\n",
    "                            height_shift_range=0.15, # Vertical shift as fraction of image height\n",
    "                            shear_range=0.15, # Intensity of shear\n",
    "                            zoom_range=0.15, # Either number or list, but specifies the range for the random zoom\n",
    "                            \n",
    "            \n",
    "                            fill_mode='nearest', # How pixels outside of the boundary are dealt with\n",
    "                            horizontal_flip=True, # Random horizontal flip of sample\n",
    "                            vertical_flip=True, # Random vertical flip of sample\n",
    "                            rescale=1./225, # Scalar multiplying the image. 1 used  \n",
    "#                             dim_ordering=K.image_dim_ordering()\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "monet1 = load_img('Claude_Monet,_Impression,_soleil_levant.jpg') # PIL image\n",
    "monet_array = img_to_array(monet1) # Convert the 2d Image to an array\n",
    "monet_array = monet_array.reshape((1,) + monet_array.shape) # Reshape to 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the purdy pictures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# t = 0\n",
    "# for batch in d_gen.flow(monet_array, batch_size=1,\n",
    "#                         save_to_dir='mod', save_prefix='monet', \n",
    "#                         save_format='jpeg'):\n",
    "#     t += 1\n",
    "#     if t > 20: # Create 20 manipulated images\n",
    "#         break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Creation of the Model\n",
    "\n",
    "*Here is where the model is created. So far the model that is being used is a rather simple three step neural network. Then going to build out a bit more*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(32,3,3, input_shape=(3,150,150)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))  \n",
    "model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "\n",
    "model.add(Convolution2D(64,2,2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Convolution2D(128,2,2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Convolution2D(256,3,3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3,3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# model = Sequential()\n",
    "# model.add(Convolution2D(32,3,3, input_shape=(3,100,100)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "\n",
    "# model.add(Convolution2D(64,3,3)\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "          \n",
    "# model.add(Convolution2D(128,2,2))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))   # Rectified Linear Unit\n",
    "model.add(Dropout(0.5))                 # Drop 50% of the Neurons\n",
    "model.add(Dense(128, activation='sigmoid'))  # Yay, sigmoids\n",
    "model.add(Dropout(0.5))                 # Drop 50% of the Neurons\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_9 (Convolution2D)  (None, 32, 148, 148)  896         convolution2d_input_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 32, 148, 148)  0           convolution2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 32, 148, 148)  0           activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_9 (MaxPooling2D)    (None, 32, 49, 49)    0           dropout_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_10 (Convolution2D) (None, 64, 48, 48)    8256        maxpooling2d_9[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 64, 48, 48)    0           convolution2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_10 (MaxPooling2D)   (None, 64, 24, 24)    0           activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_11 (Convolution2D) (None, 128, 23, 23)   32896       maxpooling2d_10[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 128, 23, 23)   0           convolution2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_11 (MaxPooling2D)   (None, 128, 11, 11)   0           activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_12 (Convolution2D) (None, 256, 9, 9)     295168      maxpooling2d_11[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 256, 9, 9)     0           convolution2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_12 (MaxPooling2D)   (None, 256, 3, 3)     0           activation_12[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 2304)          0           maxpooling2d_12[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 256)           590080      flatten_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)              (None, 256)           0           dense_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 128)           32896       dropout_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)             (None, 128)           0           dense_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 1)             129         dropout_10[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 960321\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# use the above datagen for training\n",
    "d_gen = ImageDataGenerator(rescale=1./255,\n",
    "                          shear_range=0.2,\n",
    "                          zoom_range=0.2,\n",
    "                          horizontal_flip=True)\n",
    "\n",
    "# For test only use a rescale function\n",
    "test_d_gen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 images belonging to 2 classes.\n",
      "Found 500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "monet_gen = d_gen.flow_from_directory('/home/sambozek/Desktop/Better_Data/Test/',       # Folder containing the test images\n",
    "                                     target_size = (150, 150),              # Resize to 150 * 150 pxs\n",
    "                                     batch_size = 25,                       # Reduce Noise, larger batch less noise\n",
    "                                     class_mode='binary')                   # Binary Classification Problem\n",
    "\n",
    "# bottleneck_features_train = model.predict_generator(monet_gen, 1500)\n",
    "# np.save(open('bottleneck_features_train.npy', 'w'), bottleneck_features_train)\n",
    "\n",
    "hold_out = d_gen.flow_from_directory('/home/sambozek/Desktop/Better_Data/Hold_Out/',\n",
    "                                         target_size= (150, 150),\n",
    "                                         batch_size = 25,\n",
    "                                         class_mode='binary')\n",
    "\n",
    "# bottleneck_features_validation = model.predict_generator(hold_out, 1000)\n",
    "# np.save(open('bottleneck_features_validation.npy', 'w'), bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid `Epoch comprised more than 'samples_per_epoch' samples` error need to make sure that batch size is divisible into "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "125/125 [==============================] - 8s - loss: 0.0102 - acc: 1.0000 - val_loss: 0.7050 - val_acc: 0.8520\n",
      "Epoch 2/500\n",
      "125/125 [==============================] - 5s - loss: 0.0823 - acc: 0.9760 - val_loss: 0.9423 - val_acc: 0.7920\n",
      "Epoch 3/500\n",
      "125/125 [==============================] - 5s - loss: 0.1527 - acc: 0.9520 - val_loss: 0.7525 - val_acc: 0.8280\n",
      "Epoch 4/500\n",
      "125/125 [==============================] - 5s - loss: 0.0494 - acc: 0.9840 - val_loss: 0.6371 - val_acc: 0.8360\n",
      "Epoch 5/500\n",
      "125/125 [==============================] - 5s - loss: 0.0098 - acc: 0.9920 - val_loss: 0.8252 - val_acc: 0.8200\n",
      "Epoch 6/500\n",
      "125/125 [==============================] - 5s - loss: 0.0075 - acc: 1.0000 - val_loss: 0.7644 - val_acc: 0.8240\n",
      "Epoch 7/500\n",
      "125/125 [==============================] - 5s - loss: 0.0127 - acc: 0.9920 - val_loss: 0.8290 - val_acc: 0.8040\n",
      "Epoch 8/500\n",
      "125/125 [==============================] - 5s - loss: 0.0034 - acc: 1.0000 - val_loss: 0.7399 - val_acc: 0.8360\n",
      "Epoch 9/500\n",
      "125/125 [==============================] - 5s - loss: 0.0063 - acc: 1.0000 - val_loss: 0.8899 - val_acc: 0.8240\n",
      "Epoch 10/500\n",
      "125/125 [==============================] - 5s - loss: 0.0417 - acc: 0.9840 - val_loss: 0.7494 - val_acc: 0.8320\n",
      "Epoch 11/500\n",
      "125/125 [==============================] - 5s - loss: 0.4252 - acc: 0.8880 - val_loss: 0.6687 - val_acc: 0.8560\n",
      "Epoch 12/500\n",
      "125/125 [==============================] - 5s - loss: 0.0075 - acc: 1.0000 - val_loss: 0.7854 - val_acc: 0.8400\n",
      "Epoch 13/500\n",
      "125/125 [==============================] - 5s - loss: 0.0094 - acc: 1.0000 - val_loss: 0.7138 - val_acc: 0.8520\n",
      "Epoch 14/500\n",
      "125/125 [==============================] - 5s - loss: 0.6143 - acc: 0.8480 - val_loss: 0.8521 - val_acc: 0.8200\n",
      "Epoch 15/500\n",
      "125/125 [==============================] - 5s - loss: 0.0160 - acc: 1.0000 - val_loss: 0.6893 - val_acc: 0.8360\n",
      "Epoch 16/500\n",
      "125/125 [==============================] - 5s - loss: 0.0122 - acc: 1.0000 - val_loss: 0.7623 - val_acc: 0.8320\n",
      "Epoch 17/500\n",
      "125/125 [==============================] - 5s - loss: 0.0632 - acc: 0.9920 - val_loss: 0.6647 - val_acc: 0.8640\n",
      "Epoch 18/500\n",
      "125/125 [==============================] - 5s - loss: 0.2899 - acc: 0.9280 - val_loss: 0.7364 - val_acc: 0.8320\n",
      "Epoch 19/500\n",
      "125/125 [==============================] - 5s - loss: 0.0146 - acc: 0.9920 - val_loss: 0.7167 - val_acc: 0.8320\n",
      "Epoch 20/500\n",
      "125/125 [==============================] - 5s - loss: 0.0123 - acc: 1.0000 - val_loss: 0.7628 - val_acc: 0.8320\n",
      "Epoch 21/500\n",
      "125/125 [==============================] - 5s - loss: 0.0079 - acc: 1.0000 - val_loss: 0.7287 - val_acc: 0.8600\n",
      "Epoch 22/500\n",
      "125/125 [==============================] - 5s - loss: 0.0046 - acc: 1.0000 - val_loss: 0.6580 - val_acc: 0.8400\n",
      "Epoch 23/500\n",
      "125/125 [==============================] - 5s - loss: 0.2096 - acc: 0.9440 - val_loss: 0.8629 - val_acc: 0.7960\n",
      "Epoch 24/500\n",
      "125/125 [==============================] - 5s - loss: 0.0234 - acc: 0.9920 - val_loss: 0.7032 - val_acc: 0.8520\n",
      "Epoch 25/500\n",
      "125/125 [==============================] - 5s - loss: 0.0167 - acc: 0.9920 - val_loss: 0.8795 - val_acc: 0.8040\n",
      "Epoch 26/500\n",
      "125/125 [==============================] - 5s - loss: 0.0996 - acc: 0.9680 - val_loss: 0.6728 - val_acc: 0.8400\n",
      "Epoch 27/500\n",
      "125/125 [==============================] - 5s - loss: 0.0994 - acc: 0.9680 - val_loss: 0.9261 - val_acc: 0.8160\n",
      "Epoch 28/500\n",
      "125/125 [==============================] - 5s - loss: 0.1555 - acc: 0.9520 - val_loss: 0.6015 - val_acc: 0.8600\n",
      "Epoch 29/500\n",
      "125/125 [==============================] - 5s - loss: 0.0039 - acc: 1.0000 - val_loss: 0.7238 - val_acc: 0.8200\n",
      "Epoch 30/500\n",
      "125/125 [==============================] - 5s - loss: 0.0147 - acc: 0.9920 - val_loss: 0.6483 - val_acc: 0.8560\n",
      "Epoch 31/500\n",
      "125/125 [==============================] - 5s - loss: 0.0753 - acc: 0.9840 - val_loss: 0.8555 - val_acc: 0.8080\n",
      "Epoch 32/500\n",
      "125/125 [==============================] - 5s - loss: 0.0229 - acc: 0.9920 - val_loss: 0.6887 - val_acc: 0.8480\n",
      "Epoch 33/500\n",
      "125/125 [==============================] - 5s - loss: 0.1237 - acc: 0.9520 - val_loss: 0.8179 - val_acc: 0.8240\n",
      "Epoch 34/500\n",
      "125/125 [==============================] - 5s - loss: 0.0600 - acc: 0.9760 - val_loss: 0.7543 - val_acc: 0.8400\n",
      "Epoch 35/500\n",
      "125/125 [==============================] - 5s - loss: 0.0417 - acc: 0.9840 - val_loss: 0.6843 - val_acc: 0.8240\n",
      "Epoch 36/500\n",
      "125/125 [==============================] - 5s - loss: 0.0472 - acc: 0.9840 - val_loss: 1.6916 - val_acc: 0.6440\n",
      "Epoch 37/500\n",
      "125/125 [==============================] - 5s - loss: 0.1961 - acc: 0.9520 - val_loss: 0.6765 - val_acc: 0.8480\n",
      "Epoch 38/500\n",
      "125/125 [==============================] - 6s - loss: 0.0198 - acc: 0.9920 - val_loss: 0.6695 - val_acc: 0.8560\n",
      "Epoch 39/500\n",
      "125/125 [==============================] - 7s - loss: 0.0226 - acc: 0.9920 - val_loss: 0.7541 - val_acc: 0.8360\n",
      "Epoch 40/500\n",
      "125/125 [==============================] - 6s - loss: 0.0713 - acc: 0.9760 - val_loss: 0.6877 - val_acc: 0.8440\n",
      "Epoch 41/500\n",
      "125/125 [==============================] - 6s - loss: 0.0084 - acc: 1.0000 - val_loss: 0.6968 - val_acc: 0.8480\n",
      "Epoch 42/500\n",
      "125/125 [==============================] - 6s - loss: 0.0055 - acc: 1.0000 - val_loss: 0.6805 - val_acc: 0.8560\n",
      "Epoch 43/500\n",
      "125/125 [==============================] - 5s - loss: 0.0080 - acc: 1.0000 - val_loss: 0.8213 - val_acc: 0.8200\n",
      "Epoch 44/500\n",
      "125/125 [==============================] - 5s - loss: 0.0032 - acc: 1.0000 - val_loss: 0.7797 - val_acc: 0.8400\n",
      "Epoch 45/500\n",
      "125/125 [==============================] - 6s - loss: 0.0153 - acc: 0.9920 - val_loss: 1.4757 - val_acc: 0.7120\n",
      "Epoch 46/500\n",
      "125/125 [==============================] - 6s - loss: 0.2059 - acc: 0.9440 - val_loss: 1.0334 - val_acc: 0.7760\n",
      "Epoch 47/500\n",
      "125/125 [==============================] - 5s - loss: 0.1177 - acc: 0.9840 - val_loss: 0.8004 - val_acc: 0.8280\n",
      "Epoch 48/500\n",
      "125/125 [==============================] - 6s - loss: 0.1127 - acc: 0.9760 - val_loss: 0.7594 - val_acc: 0.8240\n",
      "Epoch 49/500\n",
      "125/125 [==============================] - 6s - loss: 0.0307 - acc: 0.9920 - val_loss: 0.8028 - val_acc: 0.8160\n",
      "Epoch 50/500\n",
      "125/125 [==============================] - 5s - loss: 0.0721 - acc: 0.9760 - val_loss: 0.8146 - val_acc: 0.8240\n",
      "Epoch 51/500\n",
      "125/125 [==============================] - 5s - loss: 0.0045 - acc: 1.0000 - val_loss: 0.7383 - val_acc: 0.8520\n",
      "Epoch 52/500\n",
      "125/125 [==============================] - 5s - loss: 0.2077 - acc: 0.9520 - val_loss: 0.8025 - val_acc: 0.8240\n",
      "Epoch 53/500\n",
      "100/125 [=======================>......] - ETA: 0s - loss: 0.0403 - acc: 0.9800"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-1c47b7f027e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m                    \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhold_out\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Location of the validation data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                    \u001b[0mnb_val_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m          \u001b[1;31m# Validation samples used per epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                    verbose=1)\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'monet_weights.h5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sambozek/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, **kwargs)\u001b[0m\n\u001b[0;32m    654\u001b[0m                                         \u001b[0mnb_val_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnb_val_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m                                         \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 656\u001b[1;33m                                         max_q_size=max_q_size)\n\u001b[0m\u001b[0;32m    657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_q_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sambozek/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size)\u001b[0m\n\u001b[0;32m   1426\u001b[0m                         val_outs = self.evaluate_generator(validation_data,\n\u001b[0;32m   1427\u001b[0m                                                            \u001b[0mnb_val_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1428\u001b[1;33m                                                            max_q_size=max_q_size)\n\u001b[0m\u001b[0;32m   1429\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1430\u001b[0m                         \u001b[1;31m# no need for try/except because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sambozek/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(self, generator, val_samples, max_q_size)\u001b[0m\n\u001b[0;32m   1482\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1483\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1484\u001b[1;33m                     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1486\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(monet_gen,                  # Resize the image get from test dictionary\n",
    "                   samples_per_epoch=125,      # How many samples to use per epoch\n",
    "                   nb_epoch=500,                 # Number of epochs used\n",
    "                   validation_data = hold_out,  # Location of the validation data\n",
    "                   nb_val_samples=250,          # Validation samples used per epoch\n",
    "                   verbose=1)\n",
    "\n",
    "model.save_weights('monet_weights.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = np.load(open('bottleneck_features_train.npy'))\n",
    "# the features were saved in order, so recreating the labels is easy\n",
    "train_labels = np.array([0] * 750 + [1] * 750)\n",
    "\n",
    "validation_data = np.load(open('bottleneck_features_validation.npy'))\n",
    "validation_labels = np.array([0] * 500 + [1] * 500)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, train_labels,\n",
    "          nb_epoch=2500, batch_size=32,\n",
    "          validation_data=(validation_data, validation_labels))\n",
    "model.save_weights('bottleneck_fc_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils.visualize_util import plot\n",
    "import pydot\n",
    "plot(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "monet_model = model.to_json() # Save model to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(model.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Test : \n",
    "    * Monet:\n",
    "    * Not Monet:\n",
    "* Hold Out: **No Issues**\n",
    "    * Monet:\n",
    "    * Not Monet:\n",
    "\n",
    "*Initial running of the model found there was an issue with the image files and was given this error message:  \n",
    "**IOError: image file is truncated (39 bytes not processed)**  \n",
    "From [StackOverflow](http://bit.ly/21oXdhF) it was found that the images were likely corrupted or had unneeded trailing bytes. In order to find these truncated files used [jpeginfo](https://github.com/tjko/jpeginfo) to create a text file of the analysis of the jpegs. Output read by pandas allows for isolation of the trouble data.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!jpeginfo -c *.jpg >> Test_monet.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_monet = pd.read_csv(\"/Users/sebozek/Desktop/Test.txt\",delim_whitespace=True,header=None)\n",
    "# test_not_monet = pd.read_csv(\"/Users/sebozek/Desktop/Test.txt\",delim_whitespace=True,header=None)\n",
    "# val_monet = pd.read_csv(\"/Users/sebozek/Desktop/HoldOut.txt\",delim_whitespace=True,header=None, error_bad_lines=False)\n",
    "# val_not_monet = pd.read_csv(\"/Users/sebozek/Desktop/HoldOut_nm.txt\", delim_whitespace=True, header=None,  error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dfs = [test_monet, test_not_monet, val_monet, val_not_monet]\n",
    "\n",
    "# # More Descriptive Labels\n",
    "# for df in dfs:\n",
    "#     df.columns = [\"img_name\", \"width\", \"x\", 'height', 'bits', 'format', 'N/P', 'pixels', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for df in dfs:\n",
    "#     print df.test.unique()\n",
    "    \n",
    "# print val_not_monet.test.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# val_not_monet['img_name'][val_not_monet.test == '[ERROR]'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## To Do\n",
    "\n",
    "* Expand to try categorizing styles?\n",
    "* Figure out a few hatches\n",
    "* Try getting to above a 95% accuracy\n",
    "* Manipulate loss functions\n",
    "* Change the dropout rate and see what's going on\n",
    "* Validation\n",
    "    * ROC curve\n",
    "    * Confusion Matrix\n",
    "* What does model think is important?\n",
    "* Restricted Boltzmann Machine for Monet Replication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import previous model (that worked)\n",
    "model.load_weights('my_model_weights.h5')\n",
    "\n",
    "# Get image output\n",
    "output_1 = K.function([model.layers[0].input],\n",
    "                     [model.layers[1].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('model.txt', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_not_monet.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
