{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*This is an initial look into paths of classifying monet paintings. Due to current status may be worth even trying to do more genre based vs. artist based look into the situation.* \n",
    "  \n",
    "*The created neural network used keras on top of a theano backend. Currently using a three stage network to categorize the images.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "\n",
    "from keras.models import Sequential        # Linear stack\n",
    "\n",
    "\n",
    "from keras.layers import (                 \n",
    "    # Flattened list of layers\n",
    "    \n",
    "                          Convolution2D,   \n",
    "                            # Filters via windows of 2d input\n",
    "                          MaxPooling2D,    \n",
    "                            # Subsets image into matrices, largest value of matrix is taken\n",
    "                          Activation,      \n",
    "                            # Filters tha activate when deisred features are identified\n",
    "                          Dropout,         \n",
    "                            # Prevents overfitting by dropping based on probability 1-p\n",
    "                          Flatten,         \n",
    "                            # Layers comprising model graph\n",
    "                          Dense            \n",
    "                            # Specify input argument shape\n",
    "    \n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*In order to start it is important to begin by prcessing the data. This is a quick look into how it's done, will do quick demo*   \n",
    "  \n",
    "*Basically the image that will be looked at is taken, processed in a number of different ways (flipped, skewed, cropped, etc.). This is like turning a signature upside down for forgery. Looking at the situation through several angles.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Data Cleaning and Preparation\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "\n",
    "\n",
    "d_gen = ImageDataGenerator( featurewise_center = False, # input mean as 0 over data\n",
    "                            samplewise_center = False,  # Set sample mean to 0\n",
    "                            featurewise_std_normalization=False, # Standardize around std of dataset \n",
    "                            samplewise_std_normalization=False,  # Standardize around std of sample\n",
    "                            zca_whitening=False, # Apply ZCA whitening\n",
    "                           \n",
    "                            rotation_range=0.15,   # Random Roation within specified range\n",
    "                            width_shift_range=0.15, # Horizontal shift as a fractional of image width\n",
    "                            height_shift_range=0.15, # Vertical shift as fraction of image height\n",
    "                            shear_range=0.15, # Intensity of shear\n",
    "                            zoom_range=0.15, # Either number or list, but specifies the range for the random zoom\n",
    "                            \n",
    "                            channel_shift_range=0., # Range for random channel shifts\n",
    "                            fill_mode='nearest', # How pixels outside of the boundary are dealt with\n",
    "                            cval=0., # color value for fill_mode if that value was set to 'constant'\n",
    "                            horizontal_flip=True, # Random horizontal flip of sample\n",
    "                            vertical_flip=True, # Random vertical flip of sample\n",
    "                            rescale=1./225, # Scalar multiplying the image. 1 used  \n",
    "#                             dim_ordering=K.image_dim_ordering()\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "monet1 = load_img('Claude_Monet,_Impression,_soleil_levant.jpg') # PIL image\n",
    "monet_array = img_to_array(monet1) # Convert the 2d Image to an array\n",
    "monet_array = monet_array.reshape((1,) + monet_array.shape) # Reshape to 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the purdy pictures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d06cb4efa284>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m for batch in d_gen.flow(monet_array, batch_size=1,\n\u001b[0;32m      3\u001b[0m                         \u001b[0msave_to_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mod'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_prefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'monet'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                         save_format='jpeg'):\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Create 20 manipulated images\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sambozek/anaconda2/lib/python2.7/site-packages/keras/preprocessing/image.pyc\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_data_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_data_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mbatch_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sambozek/anaconda2/lib/python2.7/site-packages/keras/preprocessing/image.pyc\u001b[0m in \u001b[0;36mrandom_transform\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[0mtransform_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform_matrix_offset_center\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransform_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         x = apply_transform(x, transform_matrix, img_channel_index,\n\u001b[1;32m--> 346\u001b[1;33m                             fill_mode=self.fill_mode, cval=self.cval)\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchannel_shift_range\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_channel_shift\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchannel_shift_range\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_channel_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sambozek/anaconda2/lib/python2.7/site-packages/keras/preprocessing/image.pyc\u001b[0m in \u001b[0;36mapply_transform\u001b[1;34m(x, transform_matrix, channel_index, fill_mode, cval)\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[0mfinal_offset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     channel_images = [ndi.interpolation.affine_transform(x_channel, final_affine_matrix,\n\u001b[1;32m--> 108\u001b[1;33m                       final_offset, order=0, mode=fill_mode, cval=cval) for x_channel in x]\n\u001b[0m\u001b[0;32m    109\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchannel_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrollaxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannel_index\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sambozek/anaconda2/lib/python2.7/site-packages/scipy/ndimage/interpolation.pyc\u001b[0m in \u001b[0;36maffine_transform\u001b[1;34m(input, matrix, offset, output_shape, output, order, mode, cval, prefilter)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m         _geometric_transform(filtered, None, None, matrix, offset,\n\u001b[1;32m--> 419\u001b[1;33m                              output, order, mode, cval, None, None)\n\u001b[0m\u001b[0;32m    420\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mreturn_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sambozek/anaconda2/lib/python2.7/site-packages/scipy/ndimage/interpolation.pyc\u001b[0m in \u001b[0;36m_geometric_transform\u001b[1;34m(input, mapping, coordinates, matrix, offset, output, order, mode, cval, extra_arguments, extra_keywords)\u001b[0m\n\u001b[0;32m    130\u001b[0m     _nd_image.geometric_transform(\n\u001b[0;32m    131\u001b[0m         \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoordinates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         order, mode, cval, extra_arguments, extra_keywords)\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnative\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t = 0\n",
    "for batch in d_gen.flow(monet_array, batch_size=1,\n",
    "                        save_to_dir='mod', save_prefix='monet', \n",
    "                        save_format='jpeg'):\n",
    "    t += 1\n",
    "    if t > 20: # Create 20 manipulated images\n",
    "        break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Creation of the Model\n",
    "\n",
    "*Here is where the model is created. So far the model that is being used is a rather simple three step neural network. Then going to build out a bit more*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(32,3,3, input_shape=(3,150,150)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Convolution2D(32,3,3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Convolution2D(64,3,3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Convolution2D(64,3,3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(4,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))           # Rectified Linear Unit\n",
    "model.add(Dropout(0.6))                 # Drop 50% of the Neurons\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))        # Yay, sigmoids\n",
    "model.add(Activation('relu'))           # Rectified Linear Unit\n",
    "model.add(Dropout(0.5)) \n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# use the above datagen for training\n",
    "d_gen = ImageDataGenerator(rescale=1./255,\n",
    "                          shear_range=0.2,\n",
    "                          zoom_range=0.2,\n",
    "                          horizontal_flip=True)\n",
    "\n",
    "# For test only use a rescale function\n",
    "test_d_gen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4828 images belonging to 2 classes.\n",
      "Found 2163 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "monet_gen = d_gen.flow_from_directory('/home/sambozek/Desktop/Modified Data Set/Test',       # Folder containing the test images\n",
    "                                     target_size = (150, 150),              # Resize to 150 * 150 pxs\n",
    "                                     batch_size = 32,                       # Reduce Noise, larger batch less noise\n",
    "                                     class_mode='binary')                   # Binary Classification Problem\n",
    "\n",
    "hold_out = test_d_gen.flow_from_directory('/home/sambozek/Desktop/Modified Data Set/Hold Out/',\n",
    "                                         target_size= (150, 150),\n",
    "                                         batch_size = 32,\n",
    "                                         class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid `Epoch comprised more than 'samples_per_epoch' samples` error need to make sure that batch size is divisible into "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "640/640 [==============================] - 36s - loss: 1.6277 - acc: 0.7781 - val_loss: 2.1089 - val_acc: 0.3641\n",
      "Epoch 2/100\n",
      "640/640 [==============================] - 28s - loss: 1.4226 - acc: 0.8078 - val_loss: 2.1239 - val_acc: 0.3266\n",
      "Epoch 3/100\n",
      "640/640 [==============================] - 24s - loss: 1.6567 - acc: 0.7906 - val_loss: 2.2236 - val_acc: 0.3516\n",
      "Epoch 4/100\n",
      "640/640 [==============================] - 25s - loss: 1.6630 - acc: 0.8172 - val_loss: 2.3925 - val_acc: 0.3612\n",
      "Epoch 5/100\n",
      "640/640 [==============================] - 29s - loss: 1.3862 - acc: 0.8391 - val_loss: 1.9607 - val_acc: 0.3516\n",
      "Epoch 6/100\n",
      "640/640 [==============================] - 27s - loss: 1.4408 - acc: 0.8422 - val_loss: 2.9978 - val_acc: 0.3281\n",
      "Epoch 7/100\n",
      "640/640 [==============================] - 31s - loss: 1.3727 - acc: 0.8375 - val_loss: 2.7511 - val_acc: 0.3308\n",
      "Epoch 8/100\n",
      "668/640 [===============================] - 25s - loss: 1.5339 - acc: 0.8443 - val_loss: 2.5647 - val_acc: 0.3484\n",
      "Epoch 9/100\n",
      "640/640 [==============================] - 26s - loss: 1.4031 - acc: 0.8422 - val_loss: 4.4515 - val_acc: 0.3656\n",
      "Epoch 10/100\n",
      "640/640 [==============================] - 29s - loss: 1.5033 - acc: 0.8109 - val_loss: 3.3625 - val_acc: 0.3141\n",
      "Epoch 11/100\n",
      "640/640 [==============================] - 28s - loss: 1.4714 - acc: 0.8453 - val_loss: 3.5506 - val_acc: 0.3469\n",
      "Epoch 12/100\n",
      "640/640 [==============================] - 27s - loss: 1.3100 - acc: 0.8453 - val_loss: 2.7758 - val_acc: 0.3531\n",
      "Epoch 13/100\n",
      "640/640 [==============================] - 28s - loss: 1.4366 - acc: 0.8516 - val_loss: 3.0017 - val_acc: 0.3763\n",
      "Epoch 14/100\n",
      "640/640 [==============================] - 30s - loss: 1.2682 - acc: 0.8500 - val_loss: 2.8481 - val_acc: 0.3625\n",
      "Epoch 15/100\n",
      "640/640 [==============================] - 30s - loss: 1.5406 - acc: 0.8359 - val_loss: 2.1115 - val_acc: 0.3156\n",
      "Epoch 16/100\n",
      "668/640 [===============================] - 29s - loss: 1.4840 - acc: 0.8458 - val_loss: 3.7140 - val_acc: 0.3516\n",
      "Epoch 17/100\n",
      "640/640 [==============================] - 31s - loss: 1.0848 - acc: 0.8562 - val_loss: 2.7531 - val_acc: 0.3531\n",
      "Epoch 18/100\n",
      "640/640 [==============================] - 25s - loss: 1.4063 - acc: 0.8406 - val_loss: 4.9244 - val_acc: 0.3312\n",
      "Epoch 19/100\n",
      "640/640 [==============================] - 25s - loss: 1.5899 - acc: 0.8422 - val_loss: 4.0654 - val_acc: 0.3328\n",
      "Epoch 20/100\n",
      "640/640 [==============================] - 28s - loss: 1.5042 - acc: 0.8578 - val_loss: 3.9808 - val_acc: 0.3748\n",
      "Epoch 21/100\n",
      "640/640 [==============================] - 27s - loss: 1.2735 - acc: 0.8594 - val_loss: 5.1362 - val_acc: 0.3312\n",
      "Epoch 22/100\n",
      "640/640 [==============================] - 28s - loss: 1.3574 - acc: 0.8438 - val_loss: 3.2120 - val_acc: 0.3500\n",
      "Epoch 23/100\n",
      "668/640 [===============================] - 34s - loss: 1.7103 - acc: 0.8323 - val_loss: 3.1288 - val_acc: 0.3354\n",
      "Epoch 24/100\n",
      "640/640 [==============================] - 28s - loss: 1.5115 - acc: 0.8562 - val_loss: 7.1388 - val_acc: 0.3516\n",
      "Epoch 25/100\n",
      "640/640 [==============================] - 28s - loss: 1.6041 - acc: 0.8328 - val_loss: 3.2221 - val_acc: 0.3312\n",
      "Epoch 26/100\n",
      "640/640 [==============================] - 30s - loss: 1.4188 - acc: 0.8625 - val_loss: 7.9312 - val_acc: 0.3642\n",
      "Epoch 27/100\n",
      "640/640 [==============================] - 26s - loss: 1.3622 - acc: 0.8531 - val_loss: 6.0014 - val_acc: 0.3328\n",
      "Epoch 28/100\n",
      "640/640 [==============================] - 27s - loss: 1.2471 - acc: 0.8766 - val_loss: 8.7029 - val_acc: 0.3484\n",
      "Epoch 29/100\n",
      "640/640 [==============================] - 26s - loss: 1.4893 - acc: 0.8469 - val_loss: 3.6282 - val_acc: 0.3429\n",
      "Epoch 30/100\n",
      "640/640 [==============================] - 25s - loss: 1.6245 - acc: 0.8453 - val_loss: 5.5729 - val_acc: 0.3719\n",
      "Epoch 31/100\n",
      "668/640 [===============================] - 30s - loss: 1.4524 - acc: 0.8578 - val_loss: 6.8658 - val_acc: 0.3266\n",
      "Epoch 32/100\n",
      "640/640 [==============================] - 28s - loss: 1.4268 - acc: 0.8609 - val_loss: 5.8850 - val_acc: 0.3438\n",
      "Epoch 33/100\n",
      "640/640 [==============================] - 28s - loss: 1.7860 - acc: 0.8547 - val_loss: 5.5061 - val_acc: 0.3312\n",
      "Epoch 34/100\n",
      "640/640 [==============================] - 25s - loss: 1.5607 - acc: 0.8438 - val_loss: 7.3158 - val_acc: 0.3531\n",
      "Epoch 35/100\n",
      "640/640 [==============================] - 26s - loss: 1.2697 - acc: 0.8766 - val_loss: 7.3640 - val_acc: 0.3625\n",
      "Epoch 36/100\n",
      "640/640 [==============================] - 30s - loss: 1.4182 - acc: 0.8547 - val_loss: 6.2098 - val_acc: 0.2974\n",
      "Epoch 37/100\n",
      "640/640 [==============================] - 27s - loss: 1.3265 - acc: 0.8828 - val_loss: 6.2258 - val_acc: 0.3703\n",
      "Epoch 38/100\n",
      "668/640 [===============================] - 29s - loss: 1.1549 - acc: 0.8728 - val_loss: 4.7238 - val_acc: 0.3500\n",
      "Epoch 39/100\n",
      "640/640 [==============================] - 28s - loss: 1.3722 - acc: 0.8703 - val_loss: 6.1574 - val_acc: 0.3369\n",
      "Epoch 40/100\n",
      "640/640 [==============================] - 30s - loss: 1.1852 - acc: 0.8734 - val_loss: 8.0913 - val_acc: 0.3547\n",
      "Epoch 41/100\n",
      "640/640 [==============================] - 27s - loss: 1.4069 - acc: 0.8625 - val_loss: 6.4017 - val_acc: 0.3484\n",
      "Epoch 42/100\n",
      "640/640 [==============================] - 28s - loss: 1.6827 - acc: 0.8562 - val_loss: 6.7902 - val_acc: 0.3612\n",
      "Epoch 43/100\n",
      "640/640 [==============================] - 26s - loss: 1.4816 - acc: 0.8719 - val_loss: 6.2795 - val_acc: 0.3422\n",
      "Epoch 44/100\n",
      "640/640 [==============================] - 28s - loss: 1.2846 - acc: 0.8703 - val_loss: 7.1033 - val_acc: 0.3266\n",
      "Epoch 45/100\n",
      "640/640 [==============================] - 29s - loss: 1.4338 - acc: 0.8734 - val_loss: 4.7910 - val_acc: 0.3399\n",
      "Epoch 46/100\n",
      "668/640 [===============================] - 31s - loss: 1.5009 - acc: 0.8578 - val_loss: 5.8486 - val_acc: 0.3656\n",
      "Epoch 47/100\n",
      "640/640 [==============================] - 29s - loss: 1.1978 - acc: 0.8938 - val_loss: 7.3185 - val_acc: 0.3266\n",
      "Epoch 48/100\n",
      "640/640 [==============================] - 26s - loss: 1.7210 - acc: 0.8625 - val_loss: 6.3180 - val_acc: 0.3312\n",
      "Epoch 49/100\n",
      "640/640 [==============================] - 28s - loss: 1.2658 - acc: 0.8813 - val_loss: 9.1793 - val_acc: 0.3219\n",
      "Epoch 50/100\n",
      "640/640 [==============================] - 27s - loss: 1.4201 - acc: 0.8547 - val_loss: 8.1313 - val_acc: 0.3828\n",
      "Epoch 51/100\n",
      "640/640 [==============================] - 28s - loss: 1.7292 - acc: 0.8406 - val_loss: 7.2163 - val_acc: 0.3281\n",
      "Epoch 52/100\n",
      "640/640 [==============================] - 29s - loss: 1.3606 - acc: 0.8609 - val_loss: 4.8291 - val_acc: 0.3566\n",
      "Epoch 53/100\n",
      "668/640 [===============================] - 27s - loss: 1.3550 - acc: 0.8877 - val_loss: 5.1848 - val_acc: 0.3563\n",
      "Epoch 54/100\n",
      "640/640 [==============================] - 28s - loss: 1.2022 - acc: 0.8719 - val_loss: 7.4848 - val_acc: 0.3594\n",
      "Epoch 55/100\n",
      "640/640 [==============================] - 25s - loss: 1.5611 - acc: 0.8516 - val_loss: 5.6938 - val_acc: 0.3263\n",
      "Epoch 56/100\n",
      "640/640 [==============================] - 26s - loss: 1.4051 - acc: 0.8656 - val_loss: 7.7063 - val_acc: 0.3469\n",
      "Epoch 57/100\n",
      "640/640 [==============================] - 30s - loss: 1.3617 - acc: 0.8750 - val_loss: 6.7418 - val_acc: 0.3500\n",
      "Epoch 58/100\n",
      "640/640 [==============================] - 29s - loss: 1.7302 - acc: 0.8516 - val_loss: 7.4101 - val_acc: 0.3612\n",
      "Epoch 59/100\n",
      "640/640 [==============================] - 28s - loss: 1.3309 - acc: 0.8859 - val_loss: 8.9840 - val_acc: 0.3469\n",
      "Epoch 60/100\n",
      "640/640 [==============================] - 25s - loss: 1.5320 - acc: 0.8562 - val_loss: 9.0065 - val_acc: 0.3359\n",
      "Epoch 61/100\n",
      "668/640 [===============================] - 30s - loss: 1.3909 - acc: 0.8653 - val_loss: 6.1518 - val_acc: 0.3490\n",
      "Epoch 62/100\n",
      "640/640 [==============================] - 27s - loss: 1.3750 - acc: 0.8766 - val_loss: 6.9475 - val_acc: 0.3375\n",
      "Epoch 63/100\n",
      "640/640 [==============================] - 27s - loss: 1.3647 - acc: 0.8672 - val_loss: 7.4897 - val_acc: 0.3406\n",
      "Epoch 64/100\n",
      "640/640 [==============================] - 29s - loss: 1.2062 - acc: 0.8969 - val_loss: 7.1571 - val_acc: 0.3641\n",
      "Epoch 65/100\n",
      "640/640 [==============================] - 26s - loss: 1.3908 - acc: 0.8875 - val_loss: 6.6634 - val_acc: 0.3422\n",
      "Epoch 66/100\n",
      "640/640 [==============================] - 27s - loss: 1.3284 - acc: 0.8734 - val_loss: 7.0246 - val_acc: 0.3172\n",
      "Epoch 67/100\n",
      "640/640 [==============================] - 28s - loss: 1.4368 - acc: 0.8719 - val_loss: 6.0862 - val_acc: 0.3625\n",
      "Epoch 68/100\n",
      "668/640 [===============================] - 32s - loss: 1.2510 - acc: 0.8862 - val_loss: 6.8850 - val_acc: 0.3596\n",
      "Epoch 69/100\n",
      "640/640 [==============================] - 26s - loss: 1.3378 - acc: 0.8672 - val_loss: 6.9154 - val_acc: 0.3563\n",
      "Epoch 70/100\n",
      "640/640 [==============================] - 28s - loss: 1.3230 - acc: 0.8875 - val_loss: 4.5711 - val_acc: 0.3234\n",
      "Epoch 71/100\n",
      "640/640 [==============================] - 26s - loss: 1.3614 - acc: 0.8734 - val_loss: 8.1632 - val_acc: 0.3460\n",
      "Epoch 72/100\n",
      "640/640 [==============================] - 29s - loss: 1.2325 - acc: 0.8750 - val_loss: 8.1951 - val_acc: 0.3453\n",
      "Epoch 73/100\n",
      "640/640 [==============================] - 26s - loss: 1.3396 - acc: 0.8844 - val_loss: 7.2689 - val_acc: 0.3609\n",
      "Epoch 74/100\n",
      "640/640 [==============================] - 25s - loss: 1.6678 - acc: 0.8516 - val_loss: 7.1501 - val_acc: 0.3733\n",
      "Epoch 75/100\n",
      "640/640 [==============================] - 26s - loss: 1.4977 - acc: 0.8641 - val_loss: 5.0425 - val_acc: 0.3406\n",
      "Epoch 76/100\n",
      "668/640 [===============================] - 28s - loss: 1.5048 - acc: 0.8593 - val_loss: 7.2620 - val_acc: 0.3359\n",
      "Epoch 77/100\n",
      "640/640 [==============================] - 25s - loss: 1.1421 - acc: 0.9000 - val_loss: 9.8042 - val_acc: 0.3247\n",
      "Epoch 78/100\n",
      "640/640 [==============================] - 24s - loss: 1.6877 - acc: 0.8516 - val_loss: 7.2216 - val_acc: 0.3375\n",
      "Epoch 79/100\n",
      "640/640 [==============================] - 26s - loss: 1.5215 - acc: 0.8703 - val_loss: 7.4416 - val_acc: 0.3391\n",
      "Epoch 80/100\n",
      "640/640 [==============================] - 24s - loss: 1.3227 - acc: 0.8891 - val_loss: 9.3348 - val_acc: 0.3609\n",
      "Epoch 81/100\n",
      "640/640 [==============================] - 24s - loss: 1.1113 - acc: 0.8859 - val_loss: 8.9950 - val_acc: 0.3641\n",
      "Epoch 82/100\n",
      "640/640 [==============================] - 25s - loss: 1.4705 - acc: 0.8625 - val_loss: 6.5469 - val_acc: 0.3578\n",
      "Epoch 83/100\n",
      "668/640 [===============================] - 27s - loss: 1.1669 - acc: 0.8967 - val_loss: 7.1322 - val_acc: 0.3266\n",
      "Epoch 84/100\n",
      "640/640 [==============================] - 24s - loss: 1.4198 - acc: 0.8781 - val_loss: 8.0093 - val_acc: 0.3490\n",
      "Epoch 85/100\n",
      "640/640 [==============================] - 23s - loss: 1.8886 - acc: 0.8625 - val_loss: 8.0990 - val_acc: 0.3266\n",
      "Epoch 86/100\n",
      "640/640 [==============================] - 26s - loss: 1.4438 - acc: 0.8859 - val_loss: 8.2669 - val_acc: 0.3578\n",
      "Epoch 87/100\n",
      "640/640 [==============================] - 32s - loss: 1.4539 - acc: 0.8687 - val_loss: 7.1915 - val_acc: 0.3338\n",
      "Epoch 88/100\n",
      "640/640 [==============================] - 25s - loss: 1.3837 - acc: 0.8625 - val_loss: 9.8560 - val_acc: 0.3516\n",
      "Epoch 89/100\n",
      "640/640 [==============================] - 26s - loss: 1.8010 - acc: 0.8500 - val_loss: 8.8627 - val_acc: 0.3625\n",
      "Epoch 90/100\n",
      "640/640 [==============================] - 26s - loss: 1.4164 - acc: 0.8656 - val_loss: 8.1954 - val_acc: 0.3384\n",
      "Epoch 91/100\n",
      "668/640 [===============================] - 32s - loss: 1.3846 - acc: 0.8653 - val_loss: 6.6210 - val_acc: 0.3094\n",
      "Epoch 92/100\n",
      "640/640 [==============================] - 27s - loss: 1.1168 - acc: 0.9016 - val_loss: 6.7070 - val_acc: 0.3766\n",
      "Epoch 93/100\n",
      "640/640 [==============================] - 24s - loss: 1.7733 - acc: 0.8469 - val_loss: 8.8151 - val_acc: 0.3581\n",
      "Epoch 94/100\n",
      "640/640 [==============================] - 26s - loss: 1.4005 - acc: 0.8797 - val_loss: 9.3027 - val_acc: 0.3609\n",
      "Epoch 95/100\n",
      "640/640 [==============================] - 28s - loss: 1.6029 - acc: 0.8656 - val_loss: 8.4788 - val_acc: 0.3391\n",
      "Epoch 96/100\n",
      "640/640 [==============================] - 25s - loss: 1.5350 - acc: 0.8703 - val_loss: 8.8892 - val_acc: 0.3406\n",
      "Epoch 97/100\n",
      "640/640 [==============================] - 24s - loss: 1.2675 - acc: 0.8922 - val_loss: 9.0988 - val_acc: 0.3500\n",
      "Epoch 98/100\n",
      "668/640 [===============================] - 25s - loss: 1.2120 - acc: 0.8967 - val_loss: 8.5345 - val_acc: 0.3438\n",
      "Epoch 99/100\n",
      "640/640 [==============================] - 27s - loss: 1.5220 - acc: 0.8625 - val_loss: 7.7475 - val_acc: 0.3578\n",
      "Epoch 100/100\n",
      "640/640 [==============================] - 25s - loss: 1.2407 - acc: 0.8859 - val_loss: 7.4548 - val_acc: 0.3520\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(monet_gen,                  # Resize the image get from test dictionary\n",
    "                   samples_per_epoch=640,      # How many samples to use per epoch\n",
    "                   nb_epoch=100,                 # Number of epochs used\n",
    "                   validation_data = hold_out,  # Location of the validation data\n",
    "                   nb_val_samples=640,          # Validation samples used per epoch\n",
    "                   verbose=1)\n",
    "\n",
    "model.save_weights('monet_weights.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "monet_model = model.to_json() # Save model to json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Test : \n",
    "    * Monet:\n",
    "    * Not Monet:\n",
    "* Hold Out: **No Issues**\n",
    "    * Monet:\n",
    "    * Not Monet:\n",
    "\n",
    "*Initial running of the model found there was an issue with the image files and was given this error message:  \n",
    "**IOError: image file is truncated (39 bytes not processed)**  \n",
    "From [StackOverflow](http://bit.ly/21oXdhF) it was found that the images were likely corrupted or had unneeded trailing bytes. In order to find these truncated files used [jpeginfo](https://github.com/tjko/jpeginfo) to create a text file of the analysis of the jpegs. Output read by pandas allows for isolation of the trouble data.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!jpeginfo -c *.jpg >> Test_monet.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_monet = pd.read_csv(\"/Users/sebozek/Desktop/Test.txt\",delim_whitespace=True,header=None)\n",
    "# test_not_monet = pd.read_csv(\"/Users/sebozek/Desktop/Test.txt\",delim_whitespace=True,header=None)\n",
    "# val_monet = pd.read_csv(\"/Users/sebozek/Desktop/HoldOut.txt\",delim_whitespace=True,header=None, error_bad_lines=False)\n",
    "# val_not_monet = pd.read_csv(\"/Users/sebozek/Desktop/HoldOut_nm.txt\", delim_whitespace=True, header=None,  error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dfs = [test_monet, test_not_monet, val_monet, val_not_monet]\n",
    "\n",
    "# # More Descriptive Labels\n",
    "# for df in dfs:\n",
    "#     df.columns = [\"img_name\", \"width\", \"x\", 'height', 'bits', 'format', 'N/P', 'pixels', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for df in dfs:\n",
    "#     print df.test.unique()\n",
    "    \n",
    "# print val_not_monet.test.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# val_not_monet['img_name'][val_not_monet.test == '[ERROR]'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## To Do\n",
    "\n",
    "* Expand to try categorizing styles?\n",
    "* Figure out a few hatches\n",
    "* Try getting to above a 95% accuracy\n",
    "* Manipulate loss functions\n",
    "* Change the dropout rate and see what's going on\n",
    "* Validation\n",
    "    * ROC curve\n",
    "    * Confusion Matrix\n",
    "* What does model think is important?\n",
    "* Restricted Boltzmann Machine for Monet Replication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import previous model (that worked)\n",
    "model.load_weights('my_model_weights.h5')\n",
    "\n",
    "# Get image output\n",
    "output_1 = K.function([model.layers[0].input],\n",
    "                     [model.layers[1].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('model.txt', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_not_monet.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
