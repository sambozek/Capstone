{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*This is an initial look into paths of classifying monet paintings. Due to current status may be worth even trying to do more genre based vs. artist based look into the situation.* \n",
    "  \n",
    "*The created neural network used keras on top of a theano backend. Currently using a three stage network to categorize the images.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 960 (CNMeM is disabled, cuDNN not available)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "\n",
    "from keras.models import Sequential        # Linear stack\n",
    "\n",
    "\n",
    "from keras.layers import (                 \n",
    "    # Flattened list of layers\n",
    "    \n",
    "                          Convolution2D,   \n",
    "                            # Filters via windows of 2d input\n",
    "                          MaxPooling2D,    \n",
    "                            # Subsets image into matrices, largest value of matrix is taken\n",
    "                          Activation,      \n",
    "                            # Filters tha activate when deisred features are identified\n",
    "                          Dropout,         \n",
    "                            # Prevents overfitting by dropping based on probability 1-p\n",
    "                          Flatten,         \n",
    "                            # Layers comprising model graph\n",
    "                          Dense            \n",
    "                            # Specify input argument shape\n",
    "    \n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*In order to start it is important to begin by prcessing the data. This is a quick look into how it's done, will do quick demo*   \n",
    "  \n",
    "*Basically the image that will be looked at is taken, processed in a number of different ways (flipped, skewed, cropped, etc.). This is like turning a signature upside down for forgery. Looking at the situation through several angles.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Data Cleaning and Preparation\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "\n",
    "\n",
    "d_gen = ImageDataGenerator( featurewise_center = False, # input mean as 0 over data\n",
    "                            samplewise_center = False,  # Set sample mean to 0\n",
    "                            featurewise_std_normalization=False, # Standardize around std of dataset \n",
    "                            samplewise_std_normalization=False,  # Standardize around std of sample\n",
    "                            zca_whitening=False, # Apply ZCA whitening\n",
    "                           \n",
    "                            rotation_range=0.15,   # Random Roation within specified range\n",
    "                            width_shift_range=0.15, # Horizontal shift as a fractional of image width\n",
    "                            height_shift_range=0.15, # Vertical shift as fraction of image height\n",
    "                            shear_range=0.15, # Intensity of shear\n",
    "                            zoom_range=0.15, # Either number or list, but specifies the range for the random zoom\n",
    "                            \n",
    "                            channel_shift_range=0., # Range for random channel shifts\n",
    "                            fill_mode='nearest', # How pixels outside of the boundary are dealt with\n",
    "                            cval=0., # color value for fill_mode if that value was set to 'constant'\n",
    "                            horizontal_flip=True, # Random horizontal flip of sample\n",
    "                            vertical_flip=True, # Random vertical flip of sample\n",
    "                            rescale=1./225, # Scalar multiplying the image. 1 used  \n",
    "#                             dim_ordering=K.image_dim_ordering()\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "monet1 = load_img('Claude_Monet,_Impression,_soleil_levant.jpg') # PIL image\n",
    "monet_array = img_to_array(monet1) # Convert the 2d Image to an array\n",
    "monet_array = monet_array.reshape((1,) + monet_array.shape) # Reshape to 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the purdy pictures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "t = 0\n",
    "for batch in d_gen.flow(monet_array, batch_size=1,\n",
    "                        save_to_dir='mod', save_prefix='monet', \n",
    "                        save_format='jpeg'):\n",
    "    t += 1\n",
    "    if t > 20: # Create 20 manipulated images\n",
    "        break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Creation of the Model\n",
    "\n",
    "*Here is where the model is created. So far the model that is being used is a rather simple three step neural network. Then going to build out a bit more*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(32,3,3, input_shape=(3,150,150)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Convolution2D(32,3,3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Convolution2D(64,3,3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))           # Rectified Linear Unit\n",
    "model.add(Dropout(0.5))                 # Drop 50% of the Neurons\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))        # Yay, sigmoids\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# use the above datagen for training\n",
    "d_gen = ImageDataGenerator(rescale=1./255,\n",
    "                          shear_range=0.2,\n",
    "                          zoom_range=0.2,\n",
    "                          horizontal_flip=True)\n",
    "\n",
    "# For test only use a rescale function\n",
    "test_d_gen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7295 images belonging to 2 classes.\n",
      "Found 1887 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "monet_gen = d_gen.flow_from_directory('/home/sambozek/Desktop/Test/',       # Folder containing the test images\n",
    "                                     target_size = (150, 150),              # Resize to 150 * 150 pxs\n",
    "                                     batch_size = 32,                       # Reduce Noise, larger batch less noise\n",
    "                                     class_mode='binary')                   # Binary Classification Problem\n",
    "\n",
    "hold_out = test_d_gen.flow_from_directory('/home/sambozek/Desktop/Hold Out/',\n",
    "                                         target_size= (150, 150),\n",
    "                                         batch_size = 32,\n",
    "                                         class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "800/800 [==============================] - 66s - loss: 0.3921 - acc: 0.8663 - val_loss: 2.8017 - val_acc: 0.2475\n",
      "Epoch 2/25\n",
      "800/800 [==============================] - 52s - loss: 0.3113 - acc: 0.8750 - val_loss: 4.0267 - val_acc: 0.2436\n",
      "Epoch 3/25\n",
      "800/800 [==============================] - 51s - loss: 0.3585 - acc: 0.8612 - val_loss: 3.0794 - val_acc: 0.2475\n",
      "Epoch 4/25\n",
      "768/800 [===========================>..] - ETA: 0s - loss: 0.2959 - acc: 0.8698"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sambozek/anaconda2/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/sambozek/anaconda2/lib/python2.7/threading.py\", line 754, in run\n",
      "    self.__target(*self.__args, **self.__kwargs)\n",
      "  File \"/home/sambozek/anaconda2/lib/python2.7/site-packages/keras/engine/training.py\", line 404, in data_generator_task\n",
      "    generator_output = next(generator)\n",
      "  File \"/home/sambozek/anaconda2/lib/python2.7/site-packages/keras/preprocessing/image.py\", line 580, in next\n",
      "    img = load_img(os.path.join(self.directory, fname), grayscale=grayscale, target_size=self.target_size)\n",
      "  File \"/home/sambozek/anaconda2/lib/python2.7/site-packages/keras/preprocessing/image.py\", line 163, in load_img\n",
      "    img = img.convert('RGB')\n",
      "  File \"/home/sambozek/anaconda2/lib/python2.7/site-packages/PIL/Image.py\", line 842, in convert\n",
      "    self.load()\n",
      "  File \"/home/sambozek/anaconda2/lib/python2.7/site-packages/PIL/ImageFile.py\", line 218, in load\n",
      "    \"(%d bytes not processed)\" % len(b))\n",
      "IOError: image file is truncated (39 bytes not processed)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 52s - loss: 0.2953 - acc: 0.8700 - val_loss: 5.3335 - val_acc: 0.2255\n",
      "Epoch 5/25\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "output of generator should be a tuple (x, y, sample_weight) or (x, y). Found: None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-c549cc31c1b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m                    \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m                 \u001b[1;31m# Number of epochs used\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                    \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhold_out\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Location of the validation data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                    \u001b[0mnb_val_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1200\u001b[0m\u001b[1;33m,\u001b[0m          \u001b[1;31m# Validation samples used per epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m                    )\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sambozek/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, **kwargs)\u001b[0m\n\u001b[0;32m    654\u001b[0m                                         \u001b[0mnb_val_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnb_val_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m                                         \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 656\u001b[1;33m                                         max_q_size=max_q_size)\n\u001b[0m\u001b[0;32m    657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_q_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sambozek/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size)\u001b[0m\n\u001b[0;32m   1372\u001b[0m                     raise Exception('output of generator should be a tuple '\n\u001b[0;32m   1373\u001b[0m                                     \u001b[1;34m'(x, y, sample_weight) '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1374\u001b[1;33m                                     'or (x, y). Found: ' + str(generator_output))\n\u001b[0m\u001b[0;32m   1375\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m                     \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: output of generator should be a tuple (x, y, sample_weight) or (x, y). Found: None"
     ]
    }
   ],
   "source": [
    "model.fit_generator(monet_gen,                  # Resize the image get from test dictionary\n",
    "                   samples_per_epoch=800,      # How many amples to use per epoch\n",
    "                   nb_epoch=25,                 # Number of epochs used\n",
    "                   validation_data = hold_out,  # Location of the validation data\n",
    "                   nb_val_samples=1200,          # Validation samples used per epoch\n",
    "                   )\n",
    "\n",
    "model.save_weights('monet1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## To Do\n",
    "\n",
    "* Expand to try categorizing styles?\n",
    "* Figure out a few hatches\n",
    "* Try getting to above a 95% accuracy\n",
    "* Manipulate loss functions\n",
    "* Change the dropout rate and see what's going on\n",
    "* Validation\n",
    "    * ROC curve\n",
    "    * Confusion Matrix\n",
    "* What does model think is important?\n",
    "* Restricted Boltzmann Machine for Monet Replication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
